{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Real images source found, name:CelebA64 (Original)\n",
      "[INFO]: 8 different generator sources found. Names:\n",
      "a_tewst\n",
      "b_test\n",
      "CelebAHQ64\n",
      "cifar-10\n",
      "diffusionStyleGAN2\n",
      "diffusionVAE\n",
      "guidedDiffusion_IP\n",
      "noise64\n",
      "[INFO]: Comparison real-to-real (True)\n",
      "[START]: Calculating Metrics for a_tewst\n",
      "[INFO]: Start Calculation MiFID, Source = a_tewst\n",
      "[INFO]: Start Calculation FID, Source = a_tewst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n",
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "Processing samples                                                           \n",
      "Frechet Inception Distance: 39.864466274300355\n",
      "Kernel Inception Distance: 0.018253629684684705 ± 0.0006278361871706389         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: FID finished\n",
      "[INFO]: No precomputed real or generated features found. Compute from CleanFID\n",
      "[INFO]: Start Calculation Clean FID, Source = a_tewst\n",
      "[INFO]: Feature Extractor used: InceptionV3\n",
      "compute FID between two folders\n",
      "Found 1367 images in the folder d:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\org_64_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID org_64_test : 100%|██████████| 43/43 [00:09<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1180 images in the folder d:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\generated_images\\a_tewst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID a_tewst : 100%|██████████| 37/37 [00:04<00:00,  7.46it/s]\n",
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Clean FID finished\n",
      "[INFO]: Feature Extractor used: InceptionV3\n",
      "torch.Size([1180, 1367])\n",
      "[INFO]: MiFID m_tau = 0.13506710529327393\n",
      "[INFO]: MiFID finished\n",
      "[FINISHED]: Calculating Metrics for a_tewst\n",
      "{'Frechet Inception Distance': 39.864466274300355, 'Clean FID': 40.38831372936744, 'MiFID': tensor(295.1456)}\n",
      "[START]: Calculating Metrics for b_test\n",
      "[INFO]: Start Calculation MiFID, Source = b_test\n",
      "[INFO]: Start Calculation FID, Source = b_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "Processing samples                                                          \n",
      "Frechet Inception Distance: 427.20624096504804\n",
      "Kernel Inception Distance: 0.5807672156056057 ± 0.001203043931704691            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: FID finished\n",
      "[INFO]: No precomputed real or generated features found. Compute from CleanFID\n",
      "[INFO]: Start Calculation Clean FID, Source = b_test\n",
      "[INFO]: Feature Extractor used: InceptionV3\n",
      "compute FID between two folders\n",
      "[INFO]: Use precomputed real features\n",
      "Found 1027 images in the folder d:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\generated_images\\b_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID b_test : 100%|██████████| 33/33 [00:04<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Clean FID finished\n",
      "[INFO]: Feature Extractor used: InceptionV3\n",
      "torch.Size([1027, 1367])\n",
      "[INFO]: MiFID m_tau = 1.0\n",
      "[INFO]: MiFID finished\n",
      "[FINISHED]: Calculating Metrics for b_test\n",
      "{'Frechet Inception Distance': 427.20624096504804, 'Clean FID': 434.79285583030077, 'MiFID': 427.20624096504804}\n",
      "[START]: Calculating Metrics for CelebAHQ64\n",
      "[INFO]: Start Calculation MiFID, Source = CelebAHQ64\n",
      "[INFO]: Start Calculation FID, Source = CelebAHQ64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n",
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "                                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\playground.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m platform_cfg \u001b[39m=\u001b[39m PlatformConfig(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     num_worker\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m platform_manager \u001b[39m=\u001b[39m PlatformManager(real_images_path\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m\"\u001b[39m\u001b[39morg_64_test\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                                    eval_config\u001b[39m=\u001b[39meval_cfg,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                    platform_config\u001b[39m=\u001b[39mplatform_cfg)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m result_dict \u001b[39m=\u001b[39m platform_manager\u001b[39m.\u001b[39;49mcalc_metrics()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m result_dict\u001b[39m.\u001b[39mprint()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m result_dict\u001b[39m.\u001b[39mwrite_to_json(file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdict/miFID.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\framework\\platform.py:430\u001b[0m, in \u001b[0;36mPlatformManager.calc_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMiFID\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mFrechet Inception Distance\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomparator_dict:\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_fid(generator_src\u001b[39m=\u001b[39;49mgenerator_src,\n\u001b[0;32m    431\u001b[0m                      is_fid_kid_base\u001b[39m=\u001b[39;49mis_fid_kid_base,\n\u001b[0;32m    432\u001b[0m                      real_img\u001b[39m=\u001b[39;49mreal_img,\n\u001b[0;32m    433\u001b[0m                      generated_img\u001b[39m=\u001b[39;49mgenerated_img)\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhelper\u001b[39m.\u001b[39mcleanfid_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhelper\u001b[39m.\u001b[39mcleanfid_fake_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    437\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m[INFO]: No precomputed real or generated features found. Compute from CleanFID\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\framework\\platform.py:771\u001b[0m, in \u001b[0;36mPlatformManager.compute_fid\u001b[1;34m(self, generator_src, is_fid_kid_base, real_img, generated_img)\u001b[0m\n\u001b[0;32m    764\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFrechet Inception Distance\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m metric_fid \u001b[39m=\u001b[39m FID(\n\u001b[0;32m    766\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    767\u001b[0m     inception_base\u001b[39m=\u001b[39mis_fid_kid_base,\n\u001b[0;32m    768\u001b[0m     real_img\u001b[39m=\u001b[39mreal_img,\n\u001b[0;32m    769\u001b[0m     generated_img\u001b[39m=\u001b[39mgenerated_img,\n\u001b[0;32m    770\u001b[0m )\n\u001b[1;32m--> 771\u001b[0m fid \u001b[39m=\u001b[39m metric_fid\u001b[39m.\u001b[39;49mcalculate()\n\u001b[0;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomparator_dict\u001b[39m.\u001b[39mupdate({name: fid})\n\u001b[0;32m    773\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[INFO]: FID finished\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\metrics\\is_fid_kid\\metrics.py:34\u001b[0m, in \u001b[0;36mFID.calculate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minception_base\u001b[39m.\u001b[39;49mget_fid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreal_img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerated_img)\n",
      "File \u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\metrics\\is_fid_kid\\is_fid_kid.py:77\u001b[0m, in \u001b[0;36mIsFidKidBase.get_fid\u001b[1;34m(self, real_img, generated_img)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mReturn FID\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_metric_dict(real_img, generated_img)\n\u001b[0;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_dict[KEY_METRIC_FID]\n",
      "File \u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\metrics\\is_fid_kid\\is_fid_kid.py:39\u001b[0m, in \u001b[0;36mIsFidKidBase._compute_metric_dict\u001b[1;34m(self, real_img, generated_img)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_metric_dict\u001b[39m(\u001b[39mself\u001b[39m, real_img: Dataset, generated_img: Dataset) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m    Compute FID, KID based on torch-fidelity\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_dict \u001b[39m=\u001b[39m torch_fidelity\u001b[39m.\u001b[39;49mcalculate_metrics(\n\u001b[0;32m     40\u001b[0m         input1\u001b[39m=\u001b[39;49mreal_img,\n\u001b[0;32m     41\u001b[0m         input2\u001b[39m=\u001b[39;49mgenerated_img,\n\u001b[0;32m     42\u001b[0m         cuda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplatform_config\u001b[39m.\u001b[39;49mcuda,\n\u001b[0;32m     43\u001b[0m         feature_extractor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor,\n\u001b[0;32m     44\u001b[0m         fid\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     45\u001b[0m         kid\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     46\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplatform_config\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m     47\u001b[0m         kid_subsets\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_config\u001b[39m.\u001b[39;49mkid_subsets,\n\u001b[0;32m     48\u001b[0m         kid_subset_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_config\u001b[39m.\u001b[39;49mkid_subset_size,\n\u001b[0;32m     49\u001b[0m         kid_degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_config\u001b[39m.\u001b[39;49mkid_degree,\n\u001b[0;32m     50\u001b[0m         kid_coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_config\u001b[39m.\u001b[39;49mkid_coef0,\n\u001b[0;32m     51\u001b[0m     )\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\metrics.py:322\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m have_default_fe_vgg \u001b[39m=\u001b[39m have_prc\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m fe_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (have_default_fe_inception \u001b[39mand\u001b[39;00m have_default_fe_vgg):\n\u001b[0;32m    321\u001b[0m     \u001b[39m# using the same non-default feature extractor for all metrics except ppl, or using just one default extractor\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mreturn\u001b[39;00m calculate_metrics_one_feature_extractor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m out \u001b[39m=\u001b[39m {}\n\u001b[0;32m    325\u001b[0m kwargs_subset \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\metrics.py:71\u001b[0m, in \u001b[0;36mcalculate_metrics_one_feature_extractor\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m input2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     vprint(verbose, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExtracting features from input2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     featuresdict_2 \u001b[39m=\u001b[39m extract_featuresdict_from_input_id_cached(\u001b[39m2\u001b[39m, feat_extractor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m have_isc:\n\u001b[0;32m     74\u001b[0m     metric_isc \u001b[39m=\u001b[39m isc_featuresdict_to_metric(featuresdict_1, feature_layer_isc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\utils.py:407\u001b[0m, in \u001b[0;36mextract_featuresdict_from_input_id_cached\u001b[1;34m(input_id, feat_extractor, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     featuresdict \u001b[39m=\u001b[39m cache_lookup_group_recompute_all_on_any_miss(\n\u001b[0;32m    401\u001b[0m         cached_filename_prefix,\n\u001b[0;32m    402\u001b[0m         feat_extractor\u001b[39m.\u001b[39mget_requested_features_list(),\n\u001b[0;32m    403\u001b[0m         fn_recompute,\n\u001b[0;32m    404\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     featuresdict \u001b[39m=\u001b[39m fn_recompute()\n\u001b[0;32m    408\u001b[0m \u001b[39mreturn\u001b[39;00m featuresdict\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\utils.py:395\u001b[0m, in \u001b[0;36mextract_featuresdict_from_input_id_cached.<locals>.fn_recompute\u001b[1;34m()\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn_recompute\u001b[39m():\n\u001b[1;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m extract_featuresdict_from_input_id(input_id, feat_extractor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\utils.py:380\u001b[0m, in \u001b[0;36mextract_featuresdict_from_input_id\u001b[1;34m(input_id, feat_extractor, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, Dataset):\n\u001b[0;32m    379\u001b[0m     save_cpu_ram \u001b[39m=\u001b[39m get_kwarg(\u001b[39m'\u001b[39m\u001b[39msave_cpu_ram\u001b[39m\u001b[39m'\u001b[39m, kwargs)\n\u001b[1;32m--> 380\u001b[0m     featuresdict \u001b[39m=\u001b[39m get_featuresdict_from_dataset(\u001b[39minput\u001b[39;49m, feat_extractor, batch_size, cuda, save_cpu_ram, verbose)\n\u001b[0;32m    381\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     input_desc \u001b[39m=\u001b[39m prepare_input_descriptor_from_input_id(input_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\utils.py:127\u001b[0m, in \u001b[0;36mget_featuresdict_from_dataset\u001b[1;34m(input, feat_extractor, batch_size, cuda, save_cpu_ram, verbose)\u001b[0m\n\u001b[0;32m    123\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m verbose, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m'\u001b[39m, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39minput\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProcessing samples\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m t, \\\n\u001b[0;32m    126\u001b[0m         torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mfor\u001b[39;00m bid, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(dataloader):\n\u001b[0;32m    128\u001b[0m         \u001b[39mif\u001b[39;00m cuda:\n\u001b[0;32m    129\u001b[0m             batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mcuda(non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from framework.platform import PlatformManager\n",
    "from framework.configs import PlatformConfig, EvalConfig, FeatureExtractor\n",
    "import os\n",
    "\n",
    "eval_cfg = EvalConfig(\n",
    "    prd=False,\n",
    "    prd_plot=False,\n",
    "    feature_extractor=FeatureExtractor.InceptionV3,\n",
    "    c2st_knn=False,\n",
    "    fid_infinity=False,\n",
    "    is_infinity=False,\n",
    "    clean_fid=False,\n",
    "    clean_kid=False,\n",
    "    inception_score=False,\n",
    "    fid=False,\n",
    "    kid=False,\n",
    "    prc=False,\n",
    "    ls=False,\n",
    "    mifid=True,\n",
    ")\n",
    "\n",
    "platform_cfg = PlatformConfig(\n",
    "    verbose=True,\n",
    "    cuda=True,\n",
    "    save_cpu_ram=True,\n",
    "    compare_real_to_real=True,\n",
    "    num_worker=16,\n",
    ")\n",
    "\n",
    "platform_manager = PlatformManager(real_images_path=os.path.join(os.getcwd(), \"org_64_test\"),\n",
    "                                   eval_config=eval_cfg,\n",
    "                                   platform_config=platform_cfg)\n",
    "\n",
    "result_dict = platform_manager.calc_metrics()\n",
    "result_dict.print()\n",
    "result_dict.write_to_json(file=\"dict/miFID.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Real images source found, name:CelebA64 (Original)\n",
      "[INFO]: 6 different generator sources found. Names:\n",
      "CelebAHQ64\n",
      "cifar-10\n",
      "diffusionStyleGAN2\n",
      "diffusionVAE\n",
      "guidedDiffusion_IP\n",
      "noise64\n"
     ]
    }
   ],
   "source": [
    "from framework.platform import PlatformManager\n",
    "from framework.configs import PlatformConfig, EvalConfig, FeatureExtractor\n",
    "import os\n",
    "\n",
    "\n",
    "eval_cfg = EvalConfig(\n",
    "    prd=True,\n",
    "    prd_plot=True,\n",
    "    feature_extractor=FeatureExtractor.VGGFaceResNet50,\n",
    "    c2st_knn=True,\n",
    "    fid_infinity=True,\n",
    "    is_infinity=True,\n",
    "    clean_fid=True,\n",
    "    clean_kid=True,\n",
    "    inception_score=True,\n",
    "    fid=True,\n",
    "    kid=True,\n",
    "    prc=True,\n",
    "    ls=True,\n",
    ")\n",
    "\n",
    "platform_cfg = PlatformConfig(\n",
    "    verbose=True,\n",
    "    cuda=True,\n",
    "    save_cpu_ram=True,\n",
    "    compare_real_to_real=True,\n",
    "    num_worker=16,\n",
    ")\n",
    "\n",
    "platform_manager = PlatformManager(real_images_path=os.path.join(os.getcwd(), \"original_64\"),\n",
    "                                   eval_config=eval_cfg,\n",
    "                                   platform_config=platform_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9605e-08)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "r = torch.ones(20000, 2048)\n",
    "g = torch.ones(10000, 2048)\n",
    "\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "#cos = CosineSimilarity(dim=1, eps=1e-6)\n",
    "cos_distances = []\n",
    "cos_sim = pairwise_cosine_similarity(g, r)\n",
    "print(torch.min(1 - cos_sim, dim=1).values.mean().clamp(min=0.0, max=1.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202599, 3, 64, 64])\n",
      "torch.Size([50000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "\n",
    "\n",
    "real_imgs = []\n",
    "for imgs in platform_manager.get_real_images_src().get_dataloader(batch_size=64, num_worker=20):\n",
    "    real_imgs.append(imgs)\n",
    "real_imgs = torch.cat(real_imgs)\n",
    "print(real_imgs.size())\n",
    "gen_imgs = []\n",
    "for imgs in platform_manager.get_generator_src(\"guidedDiffusion_IP\").get_dataloader(batch_size=64, num_worker=20):\n",
    "    gen_imgs.append(imgs)\n",
    "gen_imgs = torch.cat(gen_imgs)\n",
    "print(gen_imgs.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 217350638388 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Dokumente\\SoftwareDev\\MasterThesis\\GIM-evaluation\\playground.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mifid \u001b[39m=\u001b[39m MemorizationInformedFrechetInceptionDistance(feature\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mifid\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mifid\u001b[39m.\u001b[39;49mupdate(real_imgs, real\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mifid\u001b[39m.\u001b[39mupdate(gen_imgs, real\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/playground.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(mifid\u001b[39m.\u001b[39mcompute())\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torchmetrics\\metric.py:467\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n\u001b[0;32m    460\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m device corresponds to the device of the input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_cpu:\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torchmetrics\\metric.py:457\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[0;32m    456\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m         update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    458\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    459\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torchmetrics\\image\\mifid.py:212\u001b[0m, in \u001b[0;36mMemorizationInformedFrechetInceptionDistance.update\u001b[1;34m(self, imgs, real)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update the state with extracted features.\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m imgs \u001b[39m=\u001b[39m (imgs \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mbyte() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39melse\u001b[39;00m imgs\n\u001b[1;32m--> 212\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minception(imgs)\n\u001b[0;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_dtype \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    214\u001b[0m features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mdouble()\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torchmetrics\\image\\fid.py:154\u001b[0m, in \u001b[0;36mNoTrainInceptionV3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    153\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass of neural network with reshaping of output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_torch_fidelity_forward(x)\n\u001b[0;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\qrno9\\miniconda3\\envs\\GIM-Eval\\lib\\site-packages\\torchmetrics\\image\\fid.py:82\u001b[0m, in \u001b[0;36mNoTrainInceptionV3._torch_fidelity_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m remaining_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_list\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     81\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_dtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> 82\u001b[0m x \u001b[39m=\u001b[39m interpolate_bilinear_2d_like_tensorflow1x(\n\u001b[0;32m     83\u001b[0m     x,\n\u001b[0;32m     84\u001b[0m     size\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mINPUT_IMAGE_SIZE, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mINPUT_IMAGE_SIZE),\n\u001b[0;32m     85\u001b[0m     align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     87\u001b[0m x \u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m \u001b[39m128\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m     89\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConv2d_1a_3x3(x)\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\interpolate_compat_tensorflow.py:139\u001b[0m, in \u001b[0;36minterpolate_bilinear_2d_like_tensorflow1x\u001b[1;34m(input, size, scale_factor, align_corners, method)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mslow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     out \u001b[39m=\u001b[39m resample_manually()\n\u001b[0;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     out \u001b[39m=\u001b[39m resample_using_grid_sample()\n",
      "File \u001b[1;32md:\\dokumente\\softwaredev\\masterthesis\\gim-evaluation\\src\\torch-fidelity\\torch_fidelity\\interpolate_compat_tensorflow.py:127\u001b[0m, in \u001b[0;36minterpolate_bilinear_2d_like_tensorflow1x.<locals>.resample_manually\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m grid_dy \u001b[39m=\u001b[39m grid_y \u001b[39m-\u001b[39m grid_y_lo\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    126\u001b[0m \u001b[39m# could be improved with index_select\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m in_00 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[:, :, grid_y_lo, :][:, :, :, grid_x_lo]\n\u001b[0;32m    128\u001b[0m in_01 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:, :, grid_y_lo, :][:, :, :, grid_x_hi]\n\u001b[0;32m    129\u001b[0m in_10 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:, :, grid_y_hi, :][:, :, :, grid_x_lo]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 217350638388 bytes."
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.mifid import MemorizationInformedFrechetInceptionDistance\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mifid = MemorizationInformedFrechetInceptionDistance(feature=2048)\n",
    "mifid.to(device)\n",
    "mifid.update(real_imgs, real=True)\n",
    "mifid.update(gen_imgs, real=False)\n",
    "print(mifid.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Generator Name: CelebAHQ64\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: 2247.051\n",
      "Kernel Inception Distance Mean: 5.213\n",
      "Kernel Inception Distance Std: 0.171\n",
      "Precision: 0.061\n",
      "Recall: 0.93\n",
      "F1 Score: 0.115\n",
      "Clean FID: 2353.092\n",
      "FID Infinity (Approx.): 2329.365\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: 5.354\n",
      "LS: 0.584\n",
      "C2ST Adaptive KNN Accuracy: 0.523\n",
      "C2ST Adaptive KNN Normalized: 0.953\n",
      "PRD F8 Max Precision: 0.829\n",
      "PRD F8 Max Recall: 0.766\n",
      "------------------------------------\n",
      "Generator Name: cifar-10\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: 6116.579\n",
      "Kernel Inception Distance Mean: 9.799\n",
      "Kernel Inception Distance Std: 0.211\n",
      "Precision: 0.374\n",
      "Recall: 0.002\n",
      "F1 Score: 0.004\n",
      "Clean FID: 5833.203\n",
      "FID Infinity (Approx.): 5826.039\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: 9.195\n",
      "LS: 0.554\n",
      "C2ST Adaptive KNN Accuracy: 0.901\n",
      "C2ST Adaptive KNN Normalized: 0.198\n",
      "PRD F8 Max Precision: 0.059\n",
      "PRD F8 Max Recall: 0.264\n",
      "------------------------------------\n",
      "Generator Name: diffusionStyleGAN2\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: 1461.74\n",
      "Kernel Inception Distance Mean: 4.05\n",
      "Kernel Inception Distance Std: 0.138\n",
      "Precision: 0.058\n",
      "Recall: 0.785\n",
      "F1 Score: 0.108\n",
      "Clean FID: 1472.098\n",
      "FID Infinity (Approx.): 1458.291\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: 4.001\n",
      "LS: 0.595\n",
      "C2ST Adaptive KNN Accuracy: 0.659\n",
      "C2ST Adaptive KNN Normalized: 0.682\n",
      "PRD F8 Max Precision: 0.898\n",
      "PRD F8 Max Recall: 0.875\n",
      "------------------------------------\n",
      "Generator Name: diffusionVAE\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: 366.403\n",
      "Kernel Inception Distance Mean: 1.032\n",
      "Kernel Inception Distance Std: 0.096\n",
      "Precision: 0.679\n",
      "Recall: 0.428\n",
      "F1 Score: 0.525\n",
      "Clean FID: 340.627\n",
      "FID Infinity (Approx.): 327.766\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: 0.927\n",
      "LS: 0.601\n",
      "C2ST Adaptive KNN Accuracy: 0.686\n",
      "C2ST Adaptive KNN Normalized: 0.628\n",
      "PRD F8 Max Precision: 0.975\n",
      "PRD F8 Max Recall: 0.981\n",
      "------------------------------------\n",
      "Generator Name: guidedDiffusion_IP\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: 140.605\n",
      "Kernel Inception Distance Mean: 0.283\n",
      "Kernel Inception Distance Std: 0.046\n",
      "Precision: 0.704\n",
      "Recall: 0.517\n",
      "F1 Score: 0.597\n",
      "Clean FID: 134.001\n",
      "FID Infinity (Approx.): 122.547\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: 0.266\n",
      "LS: 0.627\n",
      "C2ST Adaptive KNN Accuracy: 0.553\n",
      "C2ST Adaptive KNN Normalized: 0.893\n",
      "PRD F8 Max Precision: 0.99\n",
      "PRD F8 Max Recall: 0.992\n",
      "------------------------------------\n",
      "Generator Name: noise64\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: 8839.693\n",
      "Kernel Inception Distance Mean: 13.022\n",
      "Kernel Inception Distance Std: 0.213\n",
      "Precision: 0.723\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Clean FID: 8364.75\n",
      "FID Infinity (Approx.): 8365.119\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: 11.839\n",
      "LS: 0.06\n",
      "C2ST Adaptive KNN Accuracy: 0.993\n",
      "C2ST Adaptive KNN Normalized: 0.013\n",
      "PRD F8 Max Precision: 0.001\n",
      "PRD F8 Max Recall: 0.066\n",
      "------------------------------------\n",
      "Generator Name: CelebA64 (Original)\n",
      "------------------------------------\n",
      "Inception Score Mean: 1.0\n",
      "Inception Score Std: 0.0\n",
      "Frechet Inception Distance: -0.0\n",
      "Kernel Inception Distance Mean: -0.002\n",
      "Kernel Inception Distance Std: 0.016\n",
      "Precision: 0.8\n",
      "Recall: 0.801\n",
      "F1 Score: 0.801\n",
      "Clean FID: -0.0\n",
      "FID Infinity (Approx.): -3.457\n",
      "IS Infinity (Approx.): 1.0\n",
      "Clean KID: -0.002\n",
      "LS: 1.0\n",
      "C2ST Adaptive KNN Accuracy: 0.5\n",
      "C2ST Adaptive KNN Normalized: 1.0\n",
      "PRD F8 Max Precision: 1.0\n",
      "PRD F8 Max Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from framework.platform import ResultDict\n",
    "\n",
    "results = ResultDict({})\n",
    "results.read_from_json(\"dict/all_noise_celebHQ_vggface_old.json\")\n",
    "results.print(round_scores=True)\n",
    "#results.normalize_scores(print_results=True, exlude_IS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 64, 64, 3) size array saved into celeba64_train.npz\n"
     ]
    }
   ],
   "source": [
    "from framework.util import img_to_64\n",
    "\n",
    "img_to_64(path=\"D:/Dokumente/SoftwareDev/MasterThesis/CelebAMask-HQ/CelebAMask-HQ/CelebA-HQ-img/\", \n",
    "          out_path=\"D:/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/generated_images/CelebAHQ64\",\n",
    "          npz=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Create and Write Noise Imgs]:: 100%|##########| 50000/50000 [00:20<00:00, 2463.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from framework.util import create_noise64_imgs\n",
    "\n",
    "create_noise64_imgs(out_path=\"D:/Dokumente/SoftwareDev/MasterThesis/GIM-evaluation/generated_images/noise64\",\n",
    "                    num_samples=50000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIM-Eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
